x
x <- read.table(file = "clipboard", sep = "t", header=TRUE)
x <- read.table(file = "clipboard")
x
n<-31
mean(x)-qnorm(0.95)*sigma/sqrt(n)
sigma<-18
n<-31
mean(x)-qnorm(0.95)*sigma/sqrt(n)
x <- c(89,84,67,114,102,103,97,139,89,81,108,108,105,112,113,71,117,142,73,135,104,128,98,111,113,129,130,97,116,105,125)
sigma<-18
n<-31
mean(x)-qnorm(0.95)*sigma/sqrt(n)
mean(x)+qnorm(0.95)*sigma/sqrt(n)
mean(x)-qt(0.995,n-1)*sd(x)/sqrt(n)
mean(x)+qt(0.995,n-1)*sd(x)/sqrt(n)
-2*qnorm(0.975)*sigma
(-70.5587/5)²
(-70.5587/5)*(-70.5587/5)
The manufacturer of a new fiberglass tire claims that its average life will
be at least 40,000 miles. To verify this claim a sample of 11 tires is
tested, with their lifetimes (in 1,000 of miles) being as follows:
### PREGUNTA 1 ###
1. The null hypothesis is as follows:
H0: ? = ?0
Which should be the value of ?0?
40000/1000 = 40
sum = c(38.2,40,38.6,39.3,36.2,39,38.9)
sum = c(38.2,40,38.6,39.3,36.2,39,38.9,39.7,43.5)
mean(sum)
mean(sum)
P4 = (mean(sum)-40)/(sd(sum)/sqrt(length(sum)))
P4
P5 = 1-pt(abs(P4),length(sum)-1)
P5 	# si P5 peque?o es P5
p1 = mean(sum)-qt(1-((1-porcent/100)/2),length(sum)-1)*(sd(sum)/sqrt(length(sum)))
p2 = mean(sum)+qt(1-((1-porcent/100)/2),length(sum)-1)*(sd(sum)/sqrt(length(sum)))
porcent = 99
p1 = mean(sum)-qt(1-((1-porcent/100)/2),length(sum)-1)*(sd(sum)/sqrt(length(sum)))
p2 = mean(sum)+qt(1-((1-porcent/100)/2),length(sum)-1)*(sd(sum)/sqrt(length(sum)))
p1
p2
sd(sum)
porcent = 99
top = sd(sum)^2 * (length(sum)-1)
risk = 1 - porcent/100
paste(top/qchisq(1-risk/2, length(sum)-1), top/qchisq(risk/2, length(sum)-1))
N = 210
k = 14
a = 98
P = k/N
riesgo = 1 - a/100
offset = qnorm(1 - riesgo/2) * sqrt(P*(1-P)/N)
paste(P - offset, P + offset)
a3 = 90
W = 7.6
ptop = 20
riesgo2 = 1 - a3/100
n = (ptop/100)*(1-ptop/100) / (((W/200) / qnorm(1 - riesgo2/2))^2)
paste(round(n))
n = (ptop/100)*(1-ptop/100) / (((W/210) / qnorm(1 - riesgo2/2))^2)
paste(round(n))
n = (ptop/100)*(1-ptop/100) / (((W/200) / qnorm(1 - riesgo2/2))^2)
paste(round(n))
n2 = 0.5^2 / (((W/200) / qnorm(1 - riesgo2/2))^2)
paste(round(n2))
ic2 = c(0.2876, 0.3748)
a2 = 98
W2 = (ic2[2] - ic2[1])/2
P2 = ic2[1] + W2
riesgo3 = 1 - a2/100
n3 = P2*(1-P2) / ((W2 / qnorm(1 - riesgo3/2))^2)
round(ceiling(n3))
n2 = 0.5^2 / (((W/200) / qnorm(1 - riesgo2/2))^2)
paste(ceiling(n2))
po.h0 = 0.08
z = (P - po.h0) / sqrt(po.h0*(1-po.h0)/N)
paste(z)
za = abs(z)
paste(2*pnorm(-za))
N = 210
k = 38
a = 90
P = k/N
riesgo = 1 - a/100
offset = qnorm(1 - riesgo/2) * sqrt(P*(1-P)/N)
paste(P - offset, P + offset)
N = 210
k = 38
a = 90
P = k/N
riesgo = 1 - a/100
offset = qnorm(1 - riesgo/2) * sqrt(P*(1-P)/N)
paste(P - offset, P + offset)
a3 = 80
W = 7.5
ptop = 30
riesgo2 = 1 - a3/100
n = (ptop/100)*(1-ptop/100) / (((W/200) / qnorm(1 - riesgo2/2))^2)
paste(round(n))
n2 = 0.5^2 / (((W/200) / qnorm(1 - riesgo2/2))^2)
paste(ceiling(n2))
# Results" un trabajo en el que se describe una experiencia similar
# al objeto de estudio de nuestro equipo. En este trabajo, los
# autores presentan una proporci?n de ?xito ($ic2$), con confianza
# $a2$%. Sin embargo, los autores han olvidado incluir el dato de
# cu?ntos captchas utilizaron. ?Puedes averiguarlo t??
p<-0.5032
Q<-64
alpha<-1-(p/2)
? dt
dt(p,Q-1)
1-dt(p,Q-1)
1-dt(p,Q)
Q<-64
alpha<-1-(p/2)
dt(alpha,Q)
1-dt(alpha,Q)
p<-0.5032
Q<-64
alpha<-1-(p/2)
1-dt(alpha,Q)
Q<-63
alpha<-1-(p/2)
1-dt(alpha,Q)
p<-0.5032
Q<-63
alpha<-1-(p/2)
1-dt(alpha,Q)
Q<-64
alpha<-1-(p/2)
1-dt(alpha,Q)
dt(22,0.95)
dt(22,0.9975)
dt(64,0.9975)
mu=61.4
sg=3.8
# se observan sobre un solo huevo.
mu=61.4
sg=3.8
### PREGUNTA 1 ###
# Para empezar, introduce un intervalo (a,b) para el peso de un solo huevo.
# Este intervalo debe cumplir que el peso de un huevo elegido al azar estar?
# entre sus l?mites con una probabilidad entre $A$% y $B$%.
A=80
B=85
risc = 1 - ((A + B) / 200)
offset1 = qnorm(1-(risc/2)) * sg
mu-offset1
mu+offset1
n1=3
A=80
B=85
offset2 = qnorm(1-(risc/2)) * (sg/sqrt(n1))
mu-offset2
mu+offset2
n2=13
offset3 = qnorm(1-(risc/2)) * (sg/sqrt(n2))
mu-offset3
mu+offset3
ic = c(60.0361, 62.7639)
C=97.5
risc2 = 1 - C/100
n = (qnorm(1 - (risc2/2)) * 2 * sg / (ic[2] - ic[1])) ^ 2
paste(round(n))
v <- c(61, 103, 51, 114, 88, 99, 120, 110, 104, 104)
### PREGUNTA 1 ###
# La l?nea gris indica la estimaci?n puntual observada del tiempo promedio
# entre dos pulsaciones para este usuario. Halle el valor estimado.
m = mean(v)
m
error = sd(v)/sqrt(length(v))
error
m_new = 80
n = length(v)
Z = (mean(v)-m_new)/(sd(v)/sqrt(n)); Z
2*(1-pt(Z,n-1)) # o b? 2*(pt(Z,n-1)) si l'anterior ?s >1
C2 = 85
riesgo2 = 1 - C2/100
numerad = sd(v)^2 * (length(v)-1)
sqrt(numerad/qchisq(1-riesgo2/2, length(v)-1)); sqrt(numerad/qchisq(riesgo2/2, length(v)-1))
C2 = 99
riesgo2 = 1 - C2/100
numerad = sd(v)^2 * (length(v)-1)
sqrt(numerad/qchisq(1-riesgo2/2, length(v)-1)); sqrt(numerad/qchisq(riesgo2/2, length(v)-1))
C2 = 50
riesgo2 = 1 - C2/100
numerad = sd(v)^2 * (length(v)-1)
sqrt(numerad/qchisq(1-riesgo2/2, length(v)-1)); sqrt(numerad/qchisq(riesgo2/2, length(v)-1))
C2 = 2
riesgo2 = 1 - C2/100
numerad = sd(v)^2 * (length(v)-1)
sqrt(numerad/qchisq(1-riesgo2/2, length(v)-1)); sqrt(numerad/qchisq(riesgo2/2, length(v)-1))
C2 = 99.9999999999999999
riesgo2 = 1 - C2/100
numerad = sd(v)^2 * (length(v)-1)
sqrt(numerad/qchisq(1-riesgo2/2, length(v)-1)); sqrt(numerad/qchisq(riesgo2/2, length(v)-1))
.
C2 = 99.9999
riesgo2 = 1 - C2/100
numerad = sd(v)^2 * (length(v)-1)
sqrt(numerad/qchisq(1-riesgo2/2, length(v)-1)); sqrt(numerad/qchisq(riesgo2/2, length(v)-1))
H: ? = 80
? = 80
varianza = 191
sum = c(78,96,86,88.5,65,91,81.5,96,66,66.5)
P1 = (mean(sum)- ?)/(sqrt(varianza/length(sum)))
P1
? = 80
PO = 80
varianza = 191
sum = c(78,96,86,88.5,65,91,81.5,96,66,66.5)
P1 = (mean(sum)- PO)/(sqrt(varianza/length(sum)))
P1
sd(sum)
PO = 80
varianza = 191
sum = c(78,96,86,88.5,65,91,81.5,96,66,66.5)
P1 = (mean(sum)- PO)/(sqrt(varianza/length(sum)))
P1
PO = 10
P4 = (mean(sum)- ?)/(sd(sum)/sqrt(length(sum)))
P4
P4 = (mean(sum)- PO)/(sd(sum)/sqrt(length(sum)))
PO = 80
P4 = (mean(sum)- PO)/(sd(sum)/sqrt(length(sum)))
P4
P1 = (mean(sum)- PO)/(sqrt(varianza/length(sum)))
P1
P5 = (1-pt(P4, length(sum)-1))
P5
P5 = (1-pt(P4, length(sum)-1))
P5
P4
conf = 0.98
C = 1-conf
L = length(sum)
S = sd(sum)
P71 = mean(sum)-qt(1-(C/2),L-1)*(S/sqrt(L))
P72 = mean(sum)+qt(1-(C/2),L-1)*(S/sqrt(L))
P71; P72
destp = 746 	# desviacio tipus de la P8
amplitud = 207 	# amplitud de IC
P8 = mostres <- ((2*1.282*destp)/amplitud)^2
P8
qnorm(0.8)
P8 = mostres <- ((2*qnorm(0.995)*destp)/amplitud)^2
P8
clean(9)
clear()
A
rm(list=ls())
set.seed (7) # per igualar els resultats de tothom (esperem!)
n <- 20
a <- 0
b <- 1
sigma.quadrat <- 0.3^2
pep <- 3
(pep <- 3)
x <- sort(runif(n, a,b))
t <- sin(2*pi*x) + rnorm(n, mean=0, sd=sqrt(sigma.quadrat))
(sample <- data.frame(input=x,target=t))
dim(sample)
attach(sample)
# Dibuixem les dades i la funció "target" (la part regular, que voldríem detectar)
plot(input,target, lwd=3, ylim = c(-1.1, 1.1))
curve (sin(2*pi*x), a, b, add=TRUE, ylim = c(-1.1, 1.1))
input
n.valid <- 1000
x.valid <- sort(runif(n.valid, a,b))
t.valid <- sin(2*pi*x.valid) + rnorm(n.valid, mean=0, sd=sqrt(sigma.quadrat))
valid.sample <- data.frame(input=x.valid,target=t.valid)
model <- glm (target ~ input, data = sample, family = gaussian)
model # ens diu que el model és f(x) = -1.606·x + 1.133
model <- glm (target ~ input, data = sample, family = gaussian) # Nota meva: family = gaussian perque errors quadrats
model # ens diu que el model és f(x) = -1.606·x + 1.133
(prediccio <- predict(model, data=sample))
abline(model,col="red", lwd=2)
(mean.square.error <- sum((target - prediccio)^2)/n)
(prediccio <- predict(model, data=sample))
abline(model,col="red", lwd=2)
(mean.square.error <- sum((target - prediccio)^2)/n)
(prediccio <- predict(model, data=sample))
abline(model,col="red", lwd=2)
(mean.square.error <- sum((target - prediccio)^2)/n)
# alternativament, glm() ens el calcula
(mean.square.error <- model$deviance/n)
model$null.deviance/n
# alternativament, glm() ens el calcula
(mean.square.error <- model$deviance/n)
model$null.deviance/n
model # ens diu que el model és f(x) = -1.606·x + 1.133
(root.mse <- sqrt(model$deviance/n))
(NMSE <- model$deviance/((n-1)*var(target)))
o
model <- glm (target ~ poly(input, 3, raw=TRUE), data = sample, family = gaussian)
summary(model)
model$coefficients
plot(input,target, lwd=3, ylim = c(-1.1, 1.1))                # dades de TR
curve (sin(2*pi*x), a, b, add=TRUE, ylim = c(-1.1, 1.1))      # part regular a modelar
points(input, predict(model), type="l", col="red", lwd=2)     # el model obtingut
(NMSE <- model$deviance/((n-1)*var(target)))
(NMSE <- model$deviance/((n-1)*var(target)))
(NMSE <- model$deviance/((n-1)*var(target)))
(NMSE <- model$deviance/((n-1)*var(target)))
# fem un cop d'ull primer a les dades de VA
plot(valid.sample$input, valid.sample$target)
# i calculem l'error
prediccions <- predict (model, newdata=valid.sample)
(NMSE.valid <- sum((valid.sample$target - prediccions)^2)/((n.valid-1)*var(valid.sample$target)))
u
p <- 1
q <- n-1
coef <- list()
model <- list()
norm.mse.train <- NULL
norm.mse.valid <- NULL
for (i in p:q)
{
model[[i]] <- glm(target ~ poly(input, i, raw=TRUE), data = sample, family = gaussian)
# desem coeficients del polinomi (del model) i els error de training i validacio
coef[[i]] <- model[[i]]$coefficients
norm.mse.train[i] <- model[[i]]$deviance/((n-1)*var(target))
prediccions <- predict (model[[i]], newdata=valid.sample)
norm.mse.valid[i] <- sum((valid.sample$target - prediccions)^2)/((n.valid-1)*var(valid.sample$target))
}
resultats <- cbind (Grau=p:q, Coeficients=coef, Error.train=norm.mse.train, Error.valid=norm.mse.valid)
par(mfrow=c(2, 3))                # això crea una graella (grid) de 2x3
graus <- c(1,2,3,4,9,19)
for (i in graus)
{
plot(input,target, lwd=3)
curve (sin(2*pi*x), a, b, add=TRUE)
abline(0,0)
points(input, predict(model[[i]]), type="l", col=25+i, lwd=2)
title (main=paste('Grau',i))
}
for (i in graus)
{
plot(valid.sample$input, valid.sample$target)
curve (sin(2*pi*x), a, b, add=TRUE, col='yellow',lwd=2)
points(valid.sample$input, predict(model[[i]], newdata=valid.sample), type="l", col=25+i, lwd=2)
title (main=paste('Grau',i))
}
(r <- data.frame(resultats[,-2]))
par(mfrow=c(1,1))
# preparar un plot buit
plot(1:20, 1:20, xlim=c(1,16), ylim=c(0,1.5), type = "n", xlab="Grau", ylab="", xaxt="n")
# omplir-lo
axis(1, at=1:16,labels=1:16, col.axis="red", las=2)
points (x=r$Grau[1:16], y=r$Error.train[1:16], type='b', pch=0)
points (x=r$Grau[1:16], y=r$Error.valid[1:16], type='b', pch=3, lwd=3)
legend(x="topleft", legend=c("Error.TR", "Error.VA"), pch=c(0, 3), lwd=c(1, 3))
coefs.table <- matrix (nrow=10, ncol=9)
for (i in 1:10)
for (j in 1:9)
coefs.table[i,j] <- coef[[j]][i]
coefs.table
coefs.table <- matrix (nrow=10, ncol=9)
for (i in 1:10)
for (j in 1:9)
coefs.table[i,j] <- coef[[j]][i]
coefs.table
n.big <- 100                       # abans eren 20, la resta és exactament igual
x <- seq(a,b,length.out=n.big)
t <- sin(2*pi*x) + rnorm(n.big, mean=0, sd=sqrt(sigma.quadrat))
big.sample <- data.frame(input=x,target=t)
attach(big.sample)
par(mfrow=c(1, 1))
plot(big.sample$input,big.sample$target, lwd=3)
model <- glm(target ~ poly(input, 9, raw=TRUE), data = big.sample, family = gaussian)
nmse.train <- model$deviance/((n.big-1)*var(target))
prediccions <- predict (model, newdata=valid.sample)
nmse.valid <- sum((valid.sample$target - prediccions)^2)/((n.big-1)*var(valid.sample$target))
curve (sin(2*pi*x), a, b, add=TRUE)
points(big.sample$input, predict(model), type="l", col="red", lwd=2)
library(MASS)
par(mfcol=c(1,1))
lambdes <- seq(0.001,0.5,0.001)
length(lambdes)
# aquest seria el model "estàndar" (sense regularitzar)
model <- glm (target ~ poly(input, 12, raw=TRUE), data = sample, family = gaussian)
# aquest seria el model "estàndar" (sense regularitzar)
model <- glm (target ~ poly(input, 12, raw=TRUE), data = sample, family = gaussian)
# aquest seria el model regularitzat
model.ridge <- lm.ridge (model, lambda = lambdes)
plot(model.ridge, lty=1:3)
select( lm.ridge(model, lambda = lambdes) )
model.final <- lm.ridge (model,lambda=0.008)
coef(model)         # M=12 (estàndar)
coef(model.final)   # M=12 (regularitzat)
sqrt(sum(coef(model)^2)) / sqrt(sum(coef(model.final)^2))
# Millor fem un plot logarítmic, per veure els ordres de magnitud
plot (log10(abs(coef(model) / coef(model.final))), xlim=c(1,13), xlab="Grau", ylab="", main="Log10 quocient", type="b")
lambdes <- seq(0.001,0.5,0.001)
length(lambdes)
# aquest seria el model "estàndar" (sense regularitzar)
model <- glm (target ~ poly(input, 12, raw=TRUE), data = sample, family = gaussian)
# aquest seria el model regularitzat
model.ridge <- lm.ridge (model, lambda = lambdes)
plot(model.ridge, lty=1:3)
select( lm.ridge(model, lambda = lambdes) )
model.final <- lm.ridge (model,lambda=0.008)
coef(model)         # M=12 (estàndar)
coef(model.final)   # M=12 (regularitzat)
sqrt(sum(coef(model)^2)) / sqrt(sum(coef(model.final)^2))
# Millor fem un plot logarítmic, per veure els ordres de magnitud
plot (log10(abs(coef(model) / coef(model.final))), xlim=c(1,13), xlab="Grau", ylab="", main="Log10 quocient", type="b")
prediccions.classic <- predict (model, newdata=valid.sample)
(NMSE.VA.classic <- sum((valid.sample$target - prediccions.classic)^2)/((n.valid-1)*var(valid.sample$target)))
(c <- setNames(coef(model.final), paste0("c_", 0:12)))
pots <- outer (X=valid.sample$input, Y=0:12, FUN="^")
prediccions.regul <- pots %*% c
(NMSE.VA.regul <- sum((valid.sample$target - prediccions.regul)^2)/((n.valid-1)*var(valid.sample$target)))
source('~/dades/7/apa/lab/APA-L1.R')
setwd("~/dades/7/md/1/DataMining-SpeedDating")
mydata <-read.csv("Speed.csv", header=TRUE,na.strings=c("","NA"))
source('~/dades/7/md/1/DataMining-SpeedDating/script0.r')
mydata2 <- mydata [(mydata$wave==5 || mydata$wave ==6)]
View(na_count)
View(mydata2)
View(mydata2)
mydata2 <- mydata [(mydata$wave==5 || mydata$wave ==6),]
mydata2 <- mydata [(mydata$wave==5 | mydata$wave ==6),]
mydata2 <- mydata [!(mydata$wave==5 | mydata$wave ==6 | mydata$wave ==7 | mydata$wave ==8 | mydata$wave ==9),]
mydata2 <- mydata [!(mydata$wave==5 | mydata$wave ==6 | mydata$wave ==7 | mydata$wave ==8 | mydata$wave ==9 | mydata$wave ==12 | mydata$wave ==13 | mydata$wave ==14 | mydata$wave ==18 | mydata$wave ==19 | mydata$wave ==20 | mydata$wave ==21),]
View(mydata2)
View(mydata2)
mydata2[, colSums(mydf != "") != 0]
mydata2[, colSums(mydata != "") != 0]
mydata2[, colSums(mydata2 != "") != 0]
mydata2[!sapply(mydata2, function(x) all(x == ""))]
mydata3 = mydata2[!sapply(mydata2, function(x) all(x == ""))]
mydata2[!sapply(mydata2, function(x) all(x == "" | x.isNA())]
mydata2[!sapply(mydata2, function(x) all(x.isNA())]
mydata3 = mydata2[!sapply(mydata2, function(x) all(is.na(x)))]
mydata3 <- mydata2[!sapply(mydata2, function(x) all(is.na(x)))]
mydata3 = mydata2[!sapply(mydata2, function(x) all(x == "NA"))]
mydata3 = mydata2[!sapply(mydata2, function(x) all(x == 1))]
mydata3 <- mydata2[!sapply(mydata2, function(x) all(x == 1))]
mydata3 <- mydata2[!sapply(mydata2, function(x) all(x == "1"))]
emptycols <- sapply(mydata2, function (k) all(is.na(k)))
mydata3 <- mydata2[!emptycols]
View(mydata3)
mydata3 <- mydata2[,colSums(is.na(mydata2))<nrow(mydata2)]
colSums(is.na(mydata2)
)
colSums(is.na(mydata2) | mydata2 == "")
colSums(is.na(mydata2))
colSums(is.na(mydata2))
colSums(is.na(mydata2) | mydata2 == "-")
nrow(mydata2$fun4_3)
nrow(mydata2$iid)
nrow(mydata3$iid)
nrow(mydata2)
is.na(mydata2$fun4_3)
count(is.na(mydata2$fun4_3))
sumCol(is.na(mydata2$fun4_3))
sum(is.na(mydata2$fun4_3))
which(is.na(mydata2$fun4_3))
which(!is.na(mydata2$fun4_3))
which(!is.na(mydata2[3729]))
mydata2[3729]
mydata2$fun4_3[3729]
sumCol(is.na(mydata2$partner)
)
sum(is.na(mydata2$partner)
)
sum(!is.na(mydata2$partner)
)
sum(is.na(mydata2$match)
)
sum(mydata2$match)
3760/sum(mydata2$match)
sum(mydata2$match)/3760
mydata3 <- subset( mydata2, select = -a )
mydata3 <- subset( mydata2, select = a )
mydata3 <- subset( mydata2, select = -c( d : b )
)
mydata3 <- subset( mydata2, select = -c( attr1_s : amb5_3 ))
length(mydata$wave == 1)
length(mydata$wave == 2)
length(mydata$wave == 3)
sumCol(mydata$wave == 3)
sum(mydata$wave == 3)
sum(mydata$wave == 1)
sum(mydata$wave == 2)
sum(mydata$wave == 3)
sum(mydata$wave == 3)
10*2
View(mydata3)

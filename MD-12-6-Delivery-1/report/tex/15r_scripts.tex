\begin{document}
\subsection{Pre-procesing}

All R scripts are based on Karina Gibert's reference scripts for data mining.

\setlength\LTleft{-2cm}\begin{verbatim}
#Install packages
install.packages("BaylorEdPsych")
install.packages("plyr")
install.packages("dplyr")

library(BaylorEdPsych)
install.packages("mvnmle")
library(mvnmle)
installed.packages("cluster")
library(cluster)
installed.packages("dplyr")
library("plyr")
library("dplyr")

# 1 - Building the original data matrix and introducing the data into
# the pre-processing tool
# NAs can be both 'NA' or empty, in this dataset
original_data <-read.csv("Speed.csv", header=TRUE,na.strings=c("","NA"))

# Checking the dataset. Visualization, basic descriptive statistics.

dim(original_data) # size
summary(original_data)
sum(is.na(original_data))/(sum(!is.na(original_data)) + sum(is.na(original_data)))*100
# "raw" type of each variable. Warning: only raw type, integers may codify categorical
# variables, we have had to manually inspect them
sapply(original_data, class) 
# Number of NAs per columns
na_count <-sapply(original_data, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count


# Also: read the Kaggle description, read the PDF telling the meaning of each variable



# 2 - Determining the working data matrix

# Rows:we are going to select the non-variations and preference scale 1-100 rounds
# in order to have coherent and clean data

selected_rows_data <- original_data [!(original_data$wave==5
    | original_data$wave ==6 | original_data$wave ==7 |
    original_data$wave ==8 | original_data$wave ==9 | 
    original_data$wave ==12 | original_data$wave ==13 | 
    original_data$wave ==14 | original_data$wave ==18 |
    original_data$wave ==19 | original_data$wave ==20 | original_data$wave ==21),]


# Columns:
# We have many, many variables. Some of them are introducing noise,
# some of them are redundant or way to concrete, some of them are out of the scope
# of our intended analysis etc (explained in doc)
# "Expert" Variable selection (justification: redundancy, scope)
# We are kipping only some identification columns, but won't analyze them, obviously
# undergra, iid and pid only for missing imputation purposes

selected_vars<-c("iid","pid","undergra","gender","round","order","match",
    "int_corr","samerace","age_o","race_o","pf_o_att","pf_o_sin",
    "pf_o_int", "pf_o_fun", "pf_o_amb","pf_o_sha",
    "dec_o","attr_o","sinc_o","intel_o",
    "fun_o","amb_o",
    "shar_o","age","field_cd","mn_sat","tuition","race",
    "imprace","income","goal","date","go_out","dec","like","imprelig")
selected_columns_data  <- selected_rows_data[ selected_vars]

#declare qualitative variables


selected_columns_data$gender <- as.factor(selected_columns_data$gender)
selected_columns_data$match <- as.factor(selected_columns_data$match)
selected_columns_data$samerace <- as.factor(selected_columns_data$samerace)
selected_columns_data$field_cd <- as.factor(selected_columns_data$field_cd)
selected_columns_data$race <- as.factor(selected_columns_data$race)
selected_columns_data$race_o <- as.factor(selected_columns_data$race_o)
selected_columns_data$goal <- as.factor(selected_columns_data$goal)
selected_columns_data$date <- as.factor(selected_columns_data$date)
selected_columns_data$go_out <- as.factor(selected_columns_data$go_out)
selected_columns_data$dec <- as.factor(selected_columns_data$dec)

#change labels of categorical values
selected_columns_data$gender <- 
    revalue(selected_columns_data$gender,c("0"="F","1"="M"))
selected_columns_data$match <- 
    revalue(selected_columns_data$match,c("0"="N","1"="Y"))
selected_columns_data$samerace <- 
    revalue(selected_columns_data$samerace,c("0"="N","1"="Y"))
selected_columns_data$field_cd <- revalue(selected_columns_data$field_cd,
    c("1"="Law","2"="Math", 
        "3"="SocialSci", "4"="MedPharma", 
        "5"="Eng","6"="Jour", "7"="HRP","8"="Econ",
        "9"="Edu","10"="Science", "11"="SocialW",
        "12" = "Undergrad", "13"="PolSci", "14"="Film",
        "15"="Art", "16"="Lan","17"="Arch", "18"="Other"))
        
selected_columns_data$race <- revalue(selected_columns_data$race,
    c("1"="Black", "2" = "Cauc","3"="Latin", "4"="Asian", "5"="Nat","6" = "Other"))
    
selected_columns_data$race_o <- revalue(selected_columns_data$race_o,
    c("1"="Black", "2" = "Cauc", "3"="Latin", "4"="Asian", "5"="Nat","6" = "Other"))
    
selected_columns_data$goal <- revalue(selected_columns_data$goal, 
    c("1"="Fun", "2"="Meet", "3"="Date", "4"="Serious", "5"="Say","6"="Other"))

selected_columns_data$date <- revalue(selected_columns_data$date,
    c("1"="SevWeek", "2"="2Week","3"="1Week","4"="2Month","5"="1Month",
        "6"="SevYear","7"="Never"))
    
selected_columns_data$go_out <- revalue(selected_columns_data$go_out,
    c("1"="SevWeek", "2"="2Week","3"="1Week","4"="2Month","5"="1Month",
        "6"="SevYear","7"="Never"))
    
selected_columns_data$dec <- revalue(selected_columns_data$dec,c("0"="N","1"="Y"))


# We realize that tuition, mn_sat and income are detected as categorical by R, and
# simple declaring them as numeric doesn't work, so we have to manually remove commas

selected_columns_data$mn_sat<-as.numeric(gsub(",","",selected_columns_data$mn_sat))
selected_columns_data$tuition<-as.numeric(gsub(",","",selected_columns_data$tuition))
selected_columns_data$income <- as.numeric(gsub(",","",selected_columns_data$income))

# So, now he have the working matrix (selected_columns_data)


# 3 - outlier detection, visualization
# Plots of all variables, manual inspection
n_plot <- c("pid","iid","met")
for(i in 1:length(selected_columns_data[1,])) {
  name <- colnames(selected_columns_data)[i]
  if(!(name %in% n_plot)){
    if (is.factor(selected_columns_data[,i])) {
      plot(selected_columns_data[,i], main=name)
    }else {
      boxplot(selected_columns_data[,i], main=name)
    }
  }
}

# And: % of NA's for each selected column

na_count_selected <-sapply(selected_columns_data,
    function(y)100*sum(length(which(is.na(y))))/nrow(selected_columns_data))
    
na_count_selected <- data.frame(na_count_selected)
na_count_selected

# So, inspecting plots, NA's and taking into account the meaning of each variable,
# we try to detect possible outliers (id's ommited)

# Please notice that by using boxplots we are not implying that all variables follow
# a Gaussian distribution, it's for for detecting "potential" outliers 
# but we won't necessarily label them as actual outliers

# gender: no outliers, no NAs, all values 0 or 1, OK.
# round: ok.
# order: ok.
# match: same as gender.
# int_corr: 1.91% missing , no outliers in boxplot

# samerace: same as gender.
# age_o: 0.53% missing, 3 values don't fit in the boxplot but are "real" ages
# race_o: 0.53% missing, it seems it has no outliers
# pf_o_att 0.96% missing. 
# dec_o: same as gender

# attr_o 1.28% missing
# sinc_o 2.34% missing
# intel_o 2.55% missing
# fun_o 3.21 % missing
# amb_o 8.9% missing
# shar_o 13.48% missing
# No outliers, but, again, fractional values. We are going to take the floor but still
# keep them as categorical.
# age: 0.53% missing, no outliers (same as age_o)
# field_cd: 1.04% missing, no outliers apparently
# mn_sat: 62.5% missing, apparently 2 potential outliers but they are not; 
# they are "real" SAT scores
#tuition: 58.0 %missing, no apparent outliers
#race 0.53% missing no outliers apparently
#imprace 0.96% missing,  There are 8 rows with value 0 it’s supposed to be a value between
# 1-10, maybe 0 is equivalent to NA? (outliners in 0)
#income 48.72% missing, no apparent outliers
#goal 0.96% missing no outliners
#date 1.44% missing no outliners
#go_out 0.96% missing no outliners
#dec 0% missing binary with no outliners
#like 1.41% missing, outliners in some intermediate values(ex: 4.5, 5.5…)
# met 3.16% missing wrong data 1 is YES and the rest NO binary? Shall we delete this
# variable?
# imprelig, 0.96 %no outliners



# ERROR DETECTION AND TREATMENT




# imprace -> 0 is not a valid scale value (1-10), we will replace all occurrences 
    (8)  by NA
selected_columns_data$imprace[selected_columns_data$imprace == 0] <- NA


# met
# We are going to delete this variable because the values are not coherent with 
# the documentation





# MISSING IMPUTATION
data_pending_missing_imputation <- selected_columns_data

# tuition and mn_sat. structural -> 0
data_pending_missing_imputation$tuition[
    is.na(data_pending_missing_imputation$undergra)] <- 0
data_pending_missing_imputation$mn_sat[
    is.na(data_pending_missing_imputation$undergra)] <- 0
data_pending_missing_imputation$university <- data_pending_missing_imputation

data_pending_missing_imputation$university<-as.numeric(
    is.na(data_pending_missing_imputation$undergra))

data_pending_missing_imputation$university<-as.factor(
    data_pending_missing_imputation$university)

#change labels of categorical values
data_pending_missing_imputation$university<-revalue(
    data_pending_missing_imputation$university, c("0"="N","1"="Y"))

data_pending_missing_imputation<-data_pending_missing_imputation[
    data_pending_missing_imputation$iid != "28" & 
    data_pending_missing_imputation$pid != "28" & 
    data_pending_missing_imputation$iid != "58" & 
    data_pending_missing_imputation$pid != "58" &
    data_pending_missing_imputation$iid != "59" &
    data_pending_missing_imputation$pid != "59", ]



# int_corr: 1.91% missing , no outliers in boxplot.in

rowswithmissingint_corr<-filter(.data = data_pending_missing_imputation,is.na(int_corr))
summary(rowswithmissingint_corr)

# After looking at all the rows with missing Data it can be seen, 
# that there are three persons, that are responsible for the missing Values:
# Person 28 is involved in the first 32 rows with missing Values(the probablity is high, 
# that this person didn't fill the Questionare right)
# Solving suggestion: Remove all rows for this person(filling the data would need # a lot of
# assumptions about the Person and bias our results)

# The similar problem accurs for Person 58 and 59.
# cutting all the rows where they accur might be the best solution (Discussion)
# These persons also didn't answer many other columns, 
# so in my opinion its the best way if we cut them

# age_o: 0.53% missing, 3 values don't fit in the boxplot but are "real" ages
# All persons that didn't talk about their age are 58 and 59 
# so if we cut them there won't be any missing values left

# race_o: 0.53% missing, it seems it has no outliers
# Same as age_o Person 58 and 59 are responsible for the missing values
# pf_o_att 0.96% missing.
# For all Values that miss in pf_o_att 
# also by cutting Person 28,58,59 all missing Values will be cut out of the data set
# dec_o: same as gender

# attr_o 1.28% missing
# Again some missing Values are related to 58 and 59 
# since we want to focus on other values now, 
# the rows with pid or iid will be cut out before continuing with the preprocessing

data_pending_missing_imputation<-as.data.frame(filter(data_pending_missing_imputation,
    data_pending_missing_imputation$pid != 28 & data_pending_missing_imputation$iid != 28 &
    data_pending_missing_imputation$pid != 58 & data_pending_missing_imputation$iid != 58 &
    data_pending_missing_imputation$pid != 59 & data_pending_missing_imputation$iid != 59))
    

# After that, there are still two types of missing values for the following attributes.
# Rows with single missing Values and rows with all of them missing. 
# I would suggest cutting out the rows with all the values missing, 
# because imputation could lead to a big bias in our test. 
# If there are no values the questionare was probably not answered.

data_pending_missing_imputation <-  as.data.frame(filter(data_pending_missing_imputation,
    !is.na(data_pending_missing_imputation$attr_o)|
    !is.na(data_pending_missing_imputation$sinc_o)|
    !is.na(data_pending_missing_imputation$intel_o)|
    !is.na(data_pending_missing_imputation$fun_o)|
    !is.na(data_pending_missing_imputation$amb_o)|
    !is.na(data_pending_missing_imputation$shar_o)))

# After that there are only 4 rows left with missing attr_o.
# For this rows this is the only missing value. The possibilities are now:

# 1.) Remove these rows (would not suggest because if we will lose the data)
# 2.) Impute values by expert opinion (could be a good shot, 
# but we aren't experts and cannot access expert opinions right now)
# 3.) Impute values with the mean of the other 5 values
# 4.) Impute values with the mean of all attr_o values
# 5.) Impute values with the median of all attr_o values related to that person
# I think the most precise Imputation would be the median of the attr_o values
# Filter the data so only the rows with the specific iid are left:

iid10 <- filter(data_pending_missing_imputation,
    data_pending_missing_imputation$iid == 10)

# Median = 8
# set value for this row 8

data_pending_missing_imputation[96,"attr_o"] <- 8

iid22 <- filter(data_pending_missing_imputation,
    data_pending_missing_imputation$iid == 22)

# Median = 7
# set value for this row 7

data_pending_missing_imputation[224,"attr_o"] <- 7

iid37 <- filter(data_pending_missing_imputation,
    data_pending_missing_imputation$iid == 37)

# Median = 7
# set value for this row 7

data_pending_missing_imputation[447,"attr_o"] <- 7

iid104 <- filter(data_pending_missing_imputation,
    data_pending_missing_imputation$iid == 104)
# Median = 10
# set value for this row 10

data_pending_missing_imputation[1440,"attr_o"] <- 10

#LITTLE TO TEST TO IMPUTE NUMERICAL VALUES

selected_vars<-c("gender","round","order","match","int_corr","samerace","age_o","race_o",
    "pf_o_att","pf_o_sin", "pf_o_int", "pf_o_fun", "pf_o_amb","pf_o_sha",
    "dec_o","attr_o","sinc_o","intel_o","fun_o","amb_o","shar_o","age","field_cd",
    "mn_sat","tuition","race","imprace","income","goal","date","go_out","dec","like",
    "imprelig", "university")
    
data_pending_missing_imputation  <- data_pending_missing_imputation[selected_vars]

littleTest <- LittleMCAR(data_pending_missing_imputation)
littleTest$amount.missing
#The rows with the most number of NA have 17 NA
#Can we erase this rows with more than 15 NA?
littleTest$data$DataSet80




# Ultimate missing imputation


addUnknown <- function(x){
  if(is.factor(x) && sum(is.na(x) > 0 )) {
    y = factor(x, levels=c(levels(x), "Unknown"))
    y[is.na(y)] <- "Unknown"
    return(y)
  }
  return(x)
}

data_pending_missing_imputation <- as.data.frame(
    lapply(data_pending_missing_imputation, addUnknown))





vars_according_NAs <- as.data.frame(lapply(data_pending_missing_imputation, 
    function(x) if (is.numeric(x)) return(sum(is.na(x))) else return(-1)))

colnames(vars_according_NAs)

fullVariables <- c()
uncompleteVars <- c()

for (var in colnames(vars_according_NAs)){
  if (vars_according_NAs[var] > 0) {
    uncompleteVars <- c(uncompleteVars,vars_according_NAs[var])
  }
  else if (vars_according_NAs[var] == 0) {
    fullVariables <- c(fullVariables,vars_according_NAs[var])
  }
    
}

fullVariables <- colnames(as.data.frame(fullVariables))

uncompleteVars <- sort(as.data.frame(uncompleteVars))

aux<-data_pending_missing_imputation[,fullVariables]
library(class)








for (k in colnames(as.data.frame(uncompleteVars))){
  aux1 <- aux[!is.na(data_pending_missing_imputation[,k]),]
  dim(aux1) 
  aux2 <- aux[is.na(data_pending_missing_imputation[,k]),]
  dim(aux2)
  
  RefValues<- data_pending_missing_imputation[!is.na
    (data_pending_missing_imputation[,k]),k]
  #Find nns for aux2
  knn.values = knn(aux1,aux2,RefValues)   
  
  #CARE: neither aux1 nor aux2 can contain NAs
  
  
  #CARE: knn.ing is generated as a factor. 
  #Be sure to retrieve the correct values
  
  data_pending_missing_imputation[is.na(data_pending_missing_imputation[,k]),k] =
    as.numeric(as.character(knn.values))
  fullVariables<-c(fullVariables, k)
  aux<-data_pending_missing_imputation[,fullVariables]
}

data_after_imputation <- data_pending_missing_imputation

# NEW VARIABLES
# We are going to create the var "difference of age"
data_after_imputation$diff_age <- abs(data_after_imputation$age - 
    data_after_imputation$age_o)


# Correct out of scale knn values
data_after_imputation$like[data_after_imputation$like < 1] <- 1


# 100 scale

data_after_imputation$pf_sum <- rowSums(data_after_imputation[,c("pf_o_att", "pf_o_sin",
    "pf_o_fun", "pf_o_int", "pf_o_amb", "pf_o_sha")])
data_after_imputation$at_o_sum <- rowSums(data_after_imputation[,c("attr_o", "sinc_o",
    "intel_o", "fun_o", "amb_o", "shar_o")])

# No rows with total = 0for pf_sum. Only one for at_o_sum. We are going to delete it.

data_after_imputation<-data_after_imputation[data_after_imputation$pf_sum != 0 & 
    data_after_imputation$at_o_sum != 0,]

# Scale: they must add up to 100

data_after_imputation$pf_o_att <-
round(data_after_imputation$pf_o_att/data_after_imputation$pf_sum*100)
data_after_imputation$pf_o_sin <-
round(data_after_imputation$pf_o_sin/data_after_imputation$pf_sum*100)
data_after_imputation$pf_o_fun <-
round(data_after_imputation$pf_o_fun/data_after_imputation$pf_sum*100)
data_after_imputation$pf_o_int <-
round(data_after_imputation$pf_o_int/data_after_imputation$pf_sum*100)
data_after_imputation$pf_o_amb <-
round(data_after_imputation$pf_o_amb/data_after_imputation$pf_sum*100)
data_after_imputation$pf_o_sha <-
round(data_after_imputation$pf_o_sha/data_after_imputation$pf_sum*100)

data_after_imputation$attr_o <-
round(data_after_imputation$attr_o/data_after_imputation$at_o_sum*100)
data_after_imputation$sinc_o <-
round(data_after_imputation$sinc_o/data_after_imputation$at_o_sum*100)
data_after_imputation$intel_o <-
round(data_after_imputation$intel_o/data_after_imputation$at_o_sum*100)
data_after_imputation$fun_o <-
round(data_after_imputation$fun_o/data_after_imputation$at_o_sum*100)
data_after_imputation$amb_o <-
round(data_after_imputation$amb_o/data_after_imputation$at_o_sum*100)
data_after_imputation$shar_o <-
round(data_after_imputation$shar_o/data_after_imputation$at_o_sum*100)

data_after_imputation$pf_sum <- NULL
data_after_imputation$at_o_sum <- NULL

write.csv(data_after_imputation,file = "SpeedClean.csv",row.names =     
    FALSE,col.names = TRUE)




\end{verbatim}

\subsection{Basic statistical and descriptive analysis}

\begin{verbatim}
# Import the Data of our cleaned dataset without any missing values

speed_data <-read.csv("SpeedClean.csv",header = TRUE)
array_attributes <- 

#Univariate Analysis

#gender:
#Counting the values of both available gender:
summary(speed_data$gender)
#   F    M 
#1827 1829 


#Match: Counting the "Matches" and "no Matches"
summary(speed_data$match)
# N    Y 
# 3027  629 
# Percentage of match = 17,2 % Y and 82,8 N


# Int_corr:
summary(speed_data$int_corr)
sd(speed_data$int_corr)
#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#-0.7000 -0.0200  0.2100  0.1948  0.4300  0.9000 
# As expected for a correlation there are values between -1 and 1,
# the extreme points arent reached, which means, that there wasn't 
# a complete match 
# and not a complete missmatch between the participents.
# We can see that the matching seems to be higher than the
# missmatching (0,9 and -0,7). 

# samerace:
summary(speed_data$samerace)
# N    Y 
# 2085 1571 
#age:
summary(speed_data$age)
# age_o:
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 21.00   24.00   26.00   26.09   28.00   39.00 
# Standard Deviation:
sd(speed_data$age)
summary(speed_data$age_o)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
# 21.0    24.0    26.0    26.1    28.0    39.0
# As we can see the values for age and age_o are the same. They should be, by 
# checking this we made sure that there wasn't a mistake in the data.
# race_o:
summary(speed_data$race_o)
# Asian Black  Cauc Latin Other 
# 756   201  2199   289   211 
# There are two big groups of races within the Speed_dating sessions:
# Asian and Cauc
barplot(summary(speed_data$race_o))
# For the following 12 variables we will calculate the basic values(Minimum, 1st 
# Quantile, Median, Mean, 3rd Quantile, Maximum and standard Derivation )
# pf_o_amb:
summary(speed_data$pf_o_amb)
with(speed_data, hist(pf_o_amb, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$pf_o_amb)
# pf_o_att:
summary(speed_data$pf_o_att)
with(speed_data, hist(pf_o_att, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$pf_o_att)

# pf_o_sin:
summary(speed_data$pf_o_sin)
with(speed_data, hist(pf_o_sin, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$pf_o_sin)
# pf_o_int:
summary(speed_data$pf_o_int)
with(speed_data, hist(pf_o_int, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$pf_o_int)
# pf_o_fun:
summary(speed_data$pf_o_fun)
with(speed_data, hist(pf_o_fun, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$pf_o_fun)

# pf_o_sha:
summary(speed_data$pf_o_sha)
with(speed_data, hist(pf_o_sha, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$pf_o_sha)
# dec_o:
summary(speed_data$dec_o)

# standard Deviation:
sd(speed_data$dec_o)
#attr_o:
summary(speed_data$attr_o)
with(speed_data, hist(attr_o, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$attr_o)
#amb_o:
summary(speed_data$amb_o)
with(speed_data, hist(amb_o, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$amb_o)
# sinc_o:
summary(speed_data$sinc_o)
with(speed_data, hist(sinc_o, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$sinc_o)
#fun_o:
summary(speed_data$fun_o)
with(speed_data, hist(fun_o, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$fun_o)
#intel_o:
summary(speed_data$intel_o)
with(speed_data, hist(intel_o, scale="frequency", breaks="Sturges", 
                                                 col="darkgray"))
# standard Deviation:
sd(speed_data$intel_o)

#shar_o:
summary(speed_data$shar_o)
with(speed_data, hist(shar_o, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$shar_o)
 # field_cd:
summary(speed_data$field_cd)
barplot(summary(speed_data$field_cd))
#mn_sat
summary(speed_data$mn_sat)
with(speed_data, hist(mn_sat, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
# standard Deviation:
sd(speed_data$mn_sat)

summary(speed_data$like)
with(speed_data, hist(like, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
sd(speed_data$like)
#Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#1.000   5.000   6.000   6.135   7.000  10.000 
#This variable represent a mark from 1 to 10 of how much do you liked your partner,
#as we can see the mean and the median are similar and close to 6 so overall there
#are a lightly positive mark.

summary(speed_data$dec)
#  N    Y 
#2097 1559
#The 46.5% of people decided to "match" the partner and the remaining 53.5%
#rejected his partner.

summary(speed_data$go_out)
#1Month   1Week  2Month   2Week   Never SevWeek SevYear 
#98     745     212    1357      36    1157      51 

barplot(summary(speed_data$go_out))
#As we can see on the plot most of the people go out every 2 or several weeks

summary(speed_data$date)

barplot(summary(speed_data$date))
#As we can see on the plot most of the people date every several years or every 2 months.

summary(speed_data$goal)
barplot(summary(speed_data$goal))
#As we can see on the plot the main goals are having fun and meeting new people.

summary(speed_data$income)
with(speed_data, hist(income, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
sd(speed_data$income)
#There are a big difference between the higher and the lower income, the standard 
#deviation is pretty high (17k). We can relate this to people who have studies 
#and people who don't.

summary(speed_data$imprelig)
with(speed_data, hist(imprelig, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
sd(speed_data$imprelig)
#In general people give little importance to religion.

summary(speed_data$imprace)
with(speed_data, hist(imprace, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
sd(speed_data$imprace)
#In general people give little importance to race.

summary(speed_data$race)
barplot(summary(speed_data$race))
#Most people have Caucastic race.

summary(speed_data$tuition)
with(speed_data, hist(tuition, scale="frequency", breaks="Sturges", 
                      col="darkgray"))
sd(speed_data$tuition)
#There are a high standard deviation (12091) because there are a lot of people
#who haven't gone to college so they didn't paid.

# Bivariate Analysis
# In the following plot will be analized how the different Groups regarding the
# field of study influence the points given in the different categories
with(speed_data, tapply(amb_o, list(field_cd), mean, na.rm=TRUE))

barplot(with(speed_data, tapply(amb_o, list(field_cd), mean, na.rm=TRUE)))
barplot(with(speed_data, tapply(attr_o, list(field_cd), mean, na.rm=TRUE)))
barplot(with(speed_data, tapply(sinc_o, list(field_cd), mean, na.rm=TRUE)))
barplot(with(speed_data, tapply(intel_o, list(field_cd), sd, na.rm=TRUE)))
barplot(with(speed_data, tapply(fun_o, list(field_cd), sd, na.rm=TRUE)))
barplot(with(speed_data, tapply(shar_o, list(field_cd), sd, na.rm=TRUE)))
# Now for the expections that people have:
barplot(with(speed_data, tapply(pf_o_sin, list(field_cd), sd, na.rm=TRUE)))
barplot(with(speed_data, tapply(pf_o_fun, list(field_cd), sd, na.rm=TRUE)))
barplot(with(speed_data, tapply(pf_o_int, list(field_cd), sd, na.rm=TRUE)))
barplot(with(speed_data, tapply(pf_o_amb, list(field_cd), sd, na.rm=TRUE)))
barplot(with(speed_data, tapply(pf_o_sha, list(field_cd), sd, na.rm=TRUE)))
barplot(with(speed_data, tapply(pf_o_att, list(field_cd), sd, na.rm=TRUE)))
# Here can be seen, how much in a particular field they value a particular Attributte
barplot(with(speed_data, tapply(amb_o, list(field_cd), sd, na.rm=TRUE)))

# Decriptive analysis after preprocessing
class(speed_data)
dim(speed_data)
n<-dim(speed_data)[1]
n
K<-dim(speed_data)[2]
K

names(speed_data)

listOfColors<-c("blueviolet","darkviolet","mediumvioletred", 
    "palevioletred","violet", "violetred", "violetred4")
listOfColors<-c("orange","blue","green", "white","yellow", "red", "violet")
listOfColors<-palette()
listOfColors<-rainbow(14)

par(ask=TRUE)

for(k in 1:K){
  if (is.factor(speed_data[,k])){ 
    frecs<-table(speed_data[,k], useNA="ifany")
    proportions<-frecs/n
    #ojo, decidir si calcular porcentages con o sin missing values
    pie(frecs, cex=0.6, main=paste("Pie of", names(speed_data)[k]))
    barplot(frecs, las=3, cex.names=0.7, main=paste("Barplot of", 
        names(speed_data)[k]), col=listOfColors)
    print(frecs)
    print(proportions)
  }else{
    hist(speed_data[,k], main=paste("Histogram of", names(speed_data)[k]))
    boxplot(speed_data[,k], horizontal=TRUE, main=paste("Boxplot of", 
        names(speed_data)[k]))
    print(summary(speed_data[,k]))
    print(paste("sd: ", sd(speed_data[,k])))
    print(paste("vc: ", sd(speed_data[,k])/mean(speed_data[,k])))
  }
}

\end{verbatim}

\subsection{PCA}

\begin{verbatim}
dd <-read.csv("SpeedClean.csv", header=TRUE)

# attach(dd)
# names(dd)

nums <- unlist(lapply(dd, is.numeric))  
dd_numeric <- dd[ , nums]
path <- "plots/9acp/"


#set a list of numerical variables

# PRINCIPAL COMPONENT ANALYSIS OF dd_numeric

pc1 <- prcomp(dd_numeric, scale=TRUE)
class(pc1)
attributes(pc1)

print(pc1)


# WHICH PERCENTAGE OF THE TOTAL INERTIA IS REPRESENTED IN SUBSPACES?

pc1$sdev
inerProj<- pc1$sdev^2
inerProj
totalIner<- sum(inerProj)
totalIner
pinerEix<- 100*inerProj/totalIner
pinerEix
barplot(pinerEix)

#Cummulated Inertia in subspaces, from first principal component to the
# 24th dimension subspace
png(paste(path,"a-cummulated-inertia-barplot.png",sep = ""))
barplot(100*cumsum(pc1$sdev[1:dim(dd_numeric)[2]]^2)/dim(dd_numeric)[2],
    main="Accumulated Inertia",xlab = "Principal components",
    ylab = "Accumulated inertia (%)",names.arg = 1:dim(dd_numeric)[2])
dev.off()
percInerAccum<-100*cumsum(pc1$sdev[1:dim(dd_numeric)[2]]^2)/dim(dd_numeric)[2]
percInerAccum

# 14th col = 79.921189 (80%)
# SELECTION OF THE SINGIFICNT DIMENSIONS (keep 80% of total inertia)
nd = 14

# STORAGE OF THE EIGENVALUES, EIGENVECTORS AND PROJECTIONS IN THE nd DIMENSIONS
Psi = pc1$x[,1:nd]

# STORAGE OF LABELS FOR INDIVIDUALS AND VARIABLES
iden = row.names(dd_numeric)
etiq = names(dd_numeric)
ze = rep(0,length(etiq)) # WE WILL NEED THIS VECTOR AFTERWARDS FOR THE GRAPHICS

for (axis_h in 1:(nd-1)) {
  for (axis_v in (axis_h+1):nd)  {
    axis_name <- paste("x-",axis_h,"_","y-",axis_v,sep = "")
    
    # PLOT OF INDIVIDUALS [APARTAT b 1]
    #select your axis
    eje1<-axis_h
    eje2<-axis_v
    
    png(paste(path,axis_name,"b-1-individuals.png",sep = ""))
    
    plot(Psi[,eje1],Psi[,eje2])
    text(Psi[,eje1],Psi[,eje2],labels=iden, cex=0.5)
    axis(side=1, pos= 0, labels = F, col="cyan")
    axis(side=3, pos= 0, labels = F, col="cyan")
    axis(side=2, pos= 0, labels = F, col="cyan")
    axis(side=4, pos= 0, labels = F, col="cyan")
    dev.off()
    
    #library(rgl)
    #png(paste(path,axis_name,"b-1-individulals-3d.png",sep = ""))
    #plot3d(Psi[,1],Psi[,2],Psi[,3])
    #dev.off()
    
    #Projection of variables [APARTAT b 2]
    
    Phi = cor(dd_numeric,Psi)
    
    #select your axis
    
    X<-Phi[,eje1]
    Y<-Phi[,eje2]
    png(paste(path,axis_name,"b-2-proj-all-nums.png",sep = ""))
    plot(Psi[,eje1],Psi[,eje2],type="n",main="Projection of numeric variables",
        xlab=paste("Component",axis_h),ylab=paste("Component",axis_v))
    axis(side=1, pos= 0, labels = F)
    axis(side=3, pos= 0, labels = F)
    axis(side=2, pos= 0, labels = F)
    axis(side=4, pos= 0, labels = F)
    arrows(ze, ze, X, Y, length = 0.07,col="blue")
    text(X,Y,labels=etiq,col="darkblue", cex=0.7)
    dev.off()
    
    
    #zooms
    png(paste(path,axis_name,"b-2-zooms-proj-all-nums.png",sep = ""))
    plot(Psi[,eje1],Psi[,eje2],type="n",xlim=c(min(X,0),max(X,0)),ylim=c(min(Y,0),
        max(Y,0)),main="Zoomed projection of numeric variables",
        xlab=paste("Component",axis_h),ylab=paste("Component",axis_v))
    axis(side=1, pos= 0, labels = F)
    axis(side=3, pos= 0, labels = F)
    axis(side=2, pos= 0, labels = F)
    axis(side=4, pos= 0, labels = F)
    arrows(ze, ze, X, Y, length = 0.07,col="blue")
    text(X,Y,labels=etiq,col="darkblue", cex=1)

    dev.off()
    
    
    
    #Now we project both cdgs of levels of a selected qualitative variable without
    #representing the individual anymore
    
    png(paste(path,axis_name,"b-2-match.png",sep = ""))
    plot(Psi[,eje1],Psi[,eje2],type="n",main="Levels of Match",xlab=paste("Component",
        axis_h),ylab=paste("Component",axis_v))
    axis(side=1, pos= 0, labels = F, col="cyan")
    axis(side=3, pos= 0, labels = F, col="cyan")
    axis(side=2, pos= 0, labels = F, col="cyan")
    axis(side=4, pos= 0, labels = F, col="cyan")
    
    #select your qualitative variable: MATCH
    varcat<-dd[,"match"]
    fdic1 = tapply(Psi[,eje1],varcat,mean)
    fdic2 = tapply(Psi[,eje2],varcat,mean)
    
    #points(fdic1,fdic2,pch=16,col="blue", labels=levels(varcat))
    text(fdic1,fdic2,labels=levels(varcat),col="blue", cex=0.7)
    dev.off()
    
    png(paste(path,axis_name,"b-2-dec.png",sep = ""))
    plot(Psi[,eje1],Psi[,eje2],type="n",main="Levels of Dec",
        xlab=paste("Component",axis_h),ylab=paste("Component",axis_v))
    axis(side=1, pos= 0, labels = F, col="cyan")
    axis(side=3, pos= 0, labels = F, col="cyan")
    axis(side=2, pos= 0, labels = F, col="cyan")
    axis(side=4, pos= 0, labels = F, col="cyan")
    
    #select your qualitative variable: DEC
    varcat<-dd[,"dec"]
    fdic1 = tapply(Psi[,eje1],varcat,mean)
    fdic2 = tapply(Psi[,eje2],varcat,mean)
    
    #points(fdic1,fdic2,pch=16,col="blue", labels=levels(varcat))
    text(fdic1,fdic2,labels=levels(varcat),col="blue", cex=0.7)
    dev.off()
    
    png(paste(path,axis_name,"b-2-dec_o.png",sep = ""))
    plot(Psi[,eje1],Psi[,eje2],type="n",main="Levels of Dec_o",
        xlab=paste("Component",axis_h),ylab=paste("Component",axis_v))
    axis(side=1, pos= 0, labels = F, col="cyan")
    axis(side=3, pos= 0, labels = F, col="cyan")
    axis(side=2, pos= 0, labels = F, col="cyan")
    axis(side=4, pos= 0, labels = F, col="cyan")
    
    #select your qualitative variable: DEC_O
    varcat<-dd[,"dec_o"]
    fdic1 = tapply(Psi[,eje1],varcat,mean)
    fdic2 = tapply(Psi[,eje2],varcat,mean)
    
    #points(fdic1,fdic2,pch=16,col="blue", labels=levels(varcat))
    text(fdic1,fdic2,labels=levels(varcat),col="blue", cex=0.7)
    dev.off()
    
    
    #all qualitative together
    png(paste(path,axis_name,"b-2-all-qual-tog.png",sep = ""))
    plot(Psi[,eje1],Psi[,eje2],type="n",main="All qualitative variables projected
        together",xlab=paste("Component",axis_h),ylab=paste("Component",axis_v))
    axis(side=1, pos= 0, labels = F, col="cyan")
    axis(side=3, pos= 0, labels = F, col="cyan")
    axis(side=2, pos= 0, labels = F, col="cyan")
    axis(side=4, pos= 0, labels = F, col="cyan")
    
    
    
    #nominal qualitative variables
    
    facts <- unlist(lapply(dd, is.factor))  
    dcat <- names(dd[ , facts])
    dcat <- dcat[dcat != "goal" & dcat !="go_out" & dcat != "date"]
    #dcat$goal <- NULL
    #dcat$go_out <- NULL
    #dcat$date <- NULL
    #divide categoricals in several graphs if joint representation saturates
    
    #build a palette with as much colors as qualitative variables
    
    #colors<-c("blue","red","green","orange","darkgreen")
    #install.packages("viridis")
    #library(viridis)
    #viridis_pal(option = "D")(length(dcat)) # n = number of colors seeked
    
    #alternative
    colors<-rainbow(length(dcat))
    
    c<-1
    for(k in dcat){
      seguentColor<-colors[c]
      fdic1 = tapply(Psi[,eje1],dd[,k],mean)
      fdic2 = tapply(Psi[,eje2],dd[,k],mean)
      
      text(fdic1,fdic2,labels=levels(dd[,k]),col=seguentColor, cex=0.6)
      c<-c+1
    }
    legend("bottomleft",dcat,pch=1,col=colors, cex=0.6)
    
    dev.off()
    
    #determine zoom level
    #use the scale factor or not depending on the position of centroids
    # ES UN FACTOR D'ESCALA PER DIBUIXAR LES FLETXES MES VISIBLES EN EL GRAFIC
    #fm = round(max(abs(Psi[,1])))
    #fm=40
    
    #scale the projected variables
    #X<-fm*U[,eje1]
    #Y<-fm*U[,eje2]
    #X<-fm*Psi[,eje1]
    #Y<-fm*Psi[,eje2]
    
    #X<-fm*Phi[,eje1]
    #Y<-fm*Phi[,eje2]
    
   
    
    png(paste(path,axis_name,"b-2-num-background.png",sep = ""))
    #represent numerical variables in background
    plot(Psi[,eje1],Psi[,eje2],type="n",xlim=c(-1,1), ylim=c(-3,1),main="All vars
        projected together, numeric vars in background",xlab=paste("Component",axis_h),
        ylab=paste("Component",axis_v))
    #plot(X,Y,type="none",xlim=c(min(X,0),max(X,0)))
    axis(side=1, pos= 0, labels = F, col="cyan")
    axis(side=3, pos= 0, labels = F, col="cyan")
    axis(side=2, pos= 0, labels = F, col="cyan")
    axis(side=4, pos= 0, labels = F, col="cyan")
    
    #add projections of numerical variables in background
    arrows(ze, ze, X, Y, length = 0.07,col="lightgray")
    text(X,Y,labels=etiq,col="gray", cex=0.7)
    
    #add centroids
    c<-1
    for(k in dcat){
      seguentColor<-colors[c]
      
      fdic1 = tapply(Psi[,eje1],dd[,k],mean)
      fdic2 = tapply(Psi[,eje2],dd[,k],mean)
      
      #points(fdic1,fdic2,pch=16,col=seguentColor, labels=levels(dd[,k]))
      text(fdic1,fdic2,labels=levels(dd[,k]),col=seguentColor, cex=0.6)
      c<-c+1
    }
    legend("bottomleft",dcat,pch=1,col=colors, cex=0.6)
    
    
    #add ordinal qualitative variables. Ensure ordering is the correct
    
    # dordi<-c(8)
    # go_out = 31, date = 30 , goal = 29
    dordi <- c(29,30,31)
    
    
    levels(dd[,dordi[1]])
    #reorder modalities(GOAL): when required
    dd[,dordi[1]] <- factor(dd[,dordi[1]], ordered=TRUE,
        levels= c("Other","Fun","Say","Meet","Date","Serious"))
    levels(dd[,dordi[1]])
    
    
    #reorder modalities(DATE): when required
    dd[,dordi[2]] <- factor(dd[,dordi[2]], ordered=TRUE,  levels= 
        c("Unknown","Never", "SevYear","1Month",
        "2Month","1Week","2Week","SevWeek"))
    levels(dd[,dordi[2]])
    
    
    #reorder modalities(GO_OUT): when required
    dd[,dordi[3]] <- factor(dd[,dordi[3]], ordered=TRUE,  levels= c("Never",
        "SevYear","1Month", "2Month","1Week","2Week","SevWeek"))
    levels(dd[,dordi[3]])
    
    
    
    c<-1
    col <- c
    for(k in dordi){
      #seguentColor<-colors[col]
      seguentColor<-colors[c]
      fdic1 = tapply(Psi[,eje1],dd[,k],mean)
      fdic2 = tapply(Psi[,eje2],dd[,k],mean)
      
      #points(fdic1,fdic2,pch=16,col=seguentColor, labels=levels(dd[,k]))
      #connect modalities of qualitative variables
      lines(fdic1,fdic2,pch=16,col=seguentColor)
      text(fdic1,fdic2,labels=levels(dd[,k]),col=seguentColor, cex=0.6)
      c<-c+1
      col<-col+1
    }
    legend("topleft",names(dd)[dordi],pch=1,col=colors[1:length(dordi)], cex=0.6)
    
    dev.off()
    
    # PROJECTION OF ILLUSTRATIVE qualitative variables on individuals' map
    # PROJECCIÓ OF INDIVIDUALS DIFFERENTIATING THE Dictamen
    # (we need a numeric Dictamen to color)
    # MATCH
    varcat=dd[,"match"]
    png(paste(path,axis_name,"b-2-match-ill-proj.png",sep = ""))
    plot(Psi[,1],Psi[,2],col=varcat)
    axis(side=1, pos= 0, labels = F, col="darkgray")
    axis(side=3, pos= 0, labels = F, col="darkgray")
    axis(side=2, pos= 0, labels = F, col="darkgray")
    axis(side=4, pos= 0, labels = F, col="darkgray")
    legend("bottomleft",levels(varcat),pch=1,col=c(1,2), cex=0.6)
    
    
    # Overproject THE CDG OF  LEVELS OF varcat
    fdic1 = tapply(Psi[,1],varcat,mean)
    fdic2 = tapply(Psi[,2],varcat,mean)
    
    text(fdic1,fdic2,labels=levels(varcat),col="cyan", cex=0.75)
    dev.off()
    # DEC
    varcat=dd[,"dec"]
    png(paste(path,axis_name,"b-2-dec-proj.png",sep = ""))
    plot(Psi[,1],Psi[,2],col=varcat)
    axis(side=1, pos= 0, labels = F, col="darkgray")
    axis(side=3, pos= 0, labels = F, col="darkgray")
    axis(side=2, pos= 0, labels = F, col="darkgray")
    axis(side=4, pos= 0, labels = F, col="darkgray")
    legend("bottomleft",levels(varcat),pch=1,col=c(1,2), cex=0.6)
    
    
    # Overproject THE CDG OF  LEVELS OF varcat
    fdic1 = tapply(Psi[,1],varcat,mean)
    fdic2 = tapply(Psi[,2],varcat,mean)
    
    text(fdic1,fdic2,labels=levels(varcat),col="cyan", cex=0.75)
    dev.off()
    
    # DEC_O
    varcat=dd[,"dec_o"]
    png(paste(path,axis_name,"b-2-ill-dec-o.png",sep = ""))
    plot(Psi[,1],Psi[,2],col=varcat)
    axis(side=1, pos= 0, labels = F, col="darkgray")
    axis(side=3, pos= 0, labels = F, col="darkgray")
    axis(side=2, pos= 0, labels = F, col="darkgray")
    axis(side=4, pos= 0, labels = F, col="darkgray")
    legend("bottomleft",levels(varcat),pch=1,col=c(1,2), cex=0.6)
    
    
    # Overproject THE CDG OF  LEVELS OF varcat
    fdic1 = tapply(Psi[,1],varcat,mean)
    fdic2 = tapply(Psi[,2],varcat,mean)
    
    text(fdic1,fdic2,labels=levels(varcat),col="cyan", cex=0.75)
    dev.off()
  }
}

\end{verbatim}

\subsection{Hierarchical clustering}

\begin{verbatim}
install.packages("ggplot2")
install.packages("cluster")
install.packages("dplyr")
install.packages("factoextra")
install.packages("VIM")

library(ggplot2)
library(cluster)
library(dplyr)
library(factoextra)
library(VIM)

#know your actual working directory
getwd();

#set your working directory
setwd("/Users/yago/Documents/Clase/DataMining-SpeedDating");

#retrieve data obtained from preprocessing
data <-read.csv("SpeedClean.csv", header=TRUE)

#Algorithm to create clusters dealing with mixed numeric and categorical variables
actives<-c(1:ncol(data))
dissimMatrix <- daisy(data[,c(2:ncol(data))], metric = "gower", stand=TRUE)
distMatrix<-dissimMatrix^2
cluster <- hclust(distMatrix,method="ward.D")
#versions noves "ward.D" i abans de plot: par(mar=rep(2,4))
# si se quejara de los margenes del plot
plot(cluster)

#this would be enough if we only had numerical variables
#cluster <- hclust(dist(as.matrix(data)),method="ward.D2")

#Calculate optimal number of clusters. Possible methods: "silhouette", "wss", 
# "gap_stat". "gap_stat not working tho"
fviz_nbclust(data, hcut, method = "silhouette") +
geom_vline(xintercept = 3, linetype = 2)

#Assign the optimal number of clusters obtained from the functions above. It can 
# also be any other number.
numClusters <- 3
clusterCut <- cutree(cluster, numClusters)


data$cluster <- clusterCut


clusterCutRect <- rect.hclust(cluster, numClusters, border="red") 
clustersTable <- table(clusterCut, data$match)
clustersMatchTable <- table(clusterCut, data$match)

#get percentage of matches (yes/(yes+no)) for each cluster
getMatchChanceForEachCluster <- function(clustersMatchTable) {
  vector <- 1:nrow(clustersMatchTable)
  for (row in 1:nrow(clustersMatchTable)) {
    N <- clustersMatchTable[row, "N"];
    Y <- clustersMatchTable[row, "Y"];
    vector[row] <- (Y/(Y+N))*100
  }
  return (vector)
}
getMatchChanceForEachCluster(clustersMatchTable)

#count how many rows/elements/entries are there in each cluster.
cluster_append <- mutate(data, cluster = clusterCut)
count(cluster_append,cluster)

\end{verbatim}

\subsection{Profiling}

\begin{verbatim}

getwd();
setwd("/Users/yago/Documents/Clase/DataMining-SpeedDating")
dd <- read.table("SpeedClean.csv",header=T, sep=";", dec='.');

names(dd)

attach(dd)


actives<-c(1:ncol(data))


#for numerical variables we'll just do the mean for each variable and cluster
numericalMeanOfEachCluster <- aggregate(data[, c(2,3,5,7,9,10,11,12,13,14,16,17,18,
    19,20,21,22,24,25,27,28,33,34,36)], list(data$cluster), mean)
numericalMeanOfEachCluster

active<-c(1,4,6,8,15,23,26,29,30,31,32,35)
Match    <- as.factor(data$match)

#createCPG(dd[,active], Tipo.trabajo)


plotConditionalTable<-function(data, res)
{
  if(ncol(data)==0)
  {
    cat("Number of columns of dataset is 0")	
    return()
  }#endif
  if(nrow(data)==0) 
  {
    cat("Number of rows of dataset is 0")  
    return()
  }#endif
  #proceed only if data frame is non empty
  
  #transform response variable into a suitable string for printing purposes
  response<-factor(res)
  
  #create an auxiliary matrix with as much rows as classes to keep the position 
  #of figures in the CPG
  nc<-length(levels(response))
  K<-dim(data)[2]
  ncells<-nc*K
  
  mat<- matrix(data=c(1:ncells),nrow= nc, ncol=K, byrow=FALSE)
  
  #ojo, que si esta buit el panell peta
  dev.off()
  layout(mat, widths= rep.int(1, K), heights= rep.int(1,nc))
  
  for (k in 1:K){
    Vnum<-data[,k]
    for(niv in levels(response)){
      print(niv)
      s<-subset(Vnum, response==niv)
      if(is.numeric(data[,k]))
      {  hist(s, main=paste(names(data)[k], niv))
        #evenctually add other summary statistics, like vc
      }else{
        barplot(table(s), las=3, cex.names=0.5,
            main=paste("Barplot of", names(data)[k]))
      }#endifelse
    }#end for niv       
  }#end for k
}#end plot conditional table      


#data do not contain the response variable

createCPG<- function(data, response)
{
  if (!is.factor(response)) 
  {
    cat("The variable ", names(response), " must be a factor" )	
  }
  else
  {
    plotConditionalTable(data, response)
  }#end else
}#endcreateCPG



#Fer gran la finestra del R
createCPG(data[,active], as.factor(data$match))

#attach(data)

#fer creixer la finestra de plots
#control - per fer menor el tipus de lletra en R
createCPG(data[,active], as.factor(clusterCut))


levels(Match) <- c("si","no")

#Calcula els valor test de la variable Xnum per totes les modalitats del factor P
ValorTestXnum <- function(Xnum,P){
  #freq dis of fac
  nk <- as.vector(table(P)); 
  n <- sum(nk); 
  #mitjanes x grups
  xk <- tapply(Xnum,P,mean);
  #valors test
  txk <- (xk-mean(Xnum))/(sd(Xnum)*sqrt((n-nk)/(n*nk))); 
  #p-values
  pxk <- pt(txk,n-1,lower.tail=F);
  for(c in 1:length(levels(as.factor(P)))){if (pxk[c]>0.5){pxk[c]<-1-pxk[c]}}
  return (pxk)
}


ValorTestXquali <- function(P,Xquali){
  taula <- table(P,Xquali);
  n <- sum(taula);
  pk <- apply(taula,1,sum)/n;
  pj <- apply(taula,2,sum)/n;
  pf <- taula/(n*pk);
  pjm <- matrix(data=pj,nrow=dim(pf)[1],ncol=dim(pf)[2]);      
  dpf <- pf - pjm; 
  dvt <- sqrt(((1-pk)/(n*pk))%*%t(pj*(1-pj))); 
  zkj <- dpf/dvt; 
  pzkj <- pnorm(zkj,lower.tail=F);
  for(c in 1:length(levels(as.factor(P)))){for (s in 1:length(levels(Xquali)))
    {if (pzkj[c,s]> 0.5){pzkj[c,s]<-1- pzkj[c,s]}}}
  return (list(rowpf=pf,vtest=zkj,pval=pzkj))
}


dades<-data
#dades<-df
K<-dim(dades)[2]
par(ask=TRUE)

P<-clusterCut #antigament c2
nc<-length(levels(as.factor(P)))
pvalk <- matrix(data=0,nrow=nc,ncol=K, dimnames=list(levels(P),names(dades)))
nameP<-"Class"
n<-dim(dades)[1]

for(k in 1:K){
  if (is.numeric(dades[,k])){ 
    print(paste("Anàlisi per classes de la Variable:", names(dades)[k]))
    
    boxplot(dades[,k]~P, main=paste("Boxplot of", names(dades)[k], "vs", nameP ),
        horizontal=TRUE)
    
    barplot(tapply(dades[[k]], P, mean),main=paste("Means of", names(dades)[k],
        "by", nameP ))
    abline(h=mean(dades[[k]]))
    legend(0,mean(dades[[k]]),"global mean",bty="n")
    print("Estadístics per groups:")
    for(s in levels(as.factor(P))) {print(summary(dades[P==s,k]))}
    o<-oneway.test(dades[,k]~P)
    print(paste("p-valueANOVA:", o$p.value))
    kw<-kruskal.test(dades[,k]~P)
    print(paste("p-value Kruskal-Wallis:", kw$p.value))
    pvalk[,k]<-ValorTestXnum(dades[,k], P)
    print("p-values ValorsTest: ")
    print(pvalk[,k])      
  }else{
    #qualitatives
    print(paste("Variable", names(dades)[k]))
    table<-table(P,dades[,k])
    #   print("Cross-table")
    #   print(table)
    rowperc<-prop.table(table,1)
    
    colperc<-prop.table(table,2)
    #  print("Distribucions condicionades a files")
    # print(rowperc)
    
    marg <- table(as.factor(P))/n
    print(append("Categories=",levels(dades[,k])))
    plot(marg,type="l",ylim=c(0,1),main=paste("Prop. of pos & neg by",names(dades)[k]))
    paleta<-rainbow(length(levels(dades[,k])))
    for(c in 1:length(levels(dades[,k]))){lines(colperc[,c],col=paleta[c]) }
    
    #with legend
    plot(marg,type="l",ylim=c(0,1),main=paste("Prop. of pos & neg by",names(dades)[k]))
    paleta<-rainbow(length(levels(dades[,k])))
    for(c in 1:length(levels(dades[,k]))){lines(colperc[,c],col=paleta[c]) }
    legend("topright", levels(dades[,k]), col=paleta, lty=2, cex=0.6)
    
    #condicionades a classes
    print(append("Categories=",levels(dades[,k])))
    plot(marg,type="n",ylim=c(0,1),main=paste("Prop. of pos & neg by",names(dades)[k]))
    paleta<-rainbow(length(levels(dades[,k])))
    for(c in 1:length(levels(dades[,k]))){lines(rowperc[,c],col=paleta[c]) }
    
    #with legend
    plot(marg,type="n",ylim=c(0,1),main=paste("Prop. of pos & neg by",names(dades)[k]))
    paleta<-rainbow(length(levels(dades[,k])))
    for(c in 1:length(levels(dades[,k]))){lines(rowperc[,c],col=paleta[c]) }
    legend("topright", levels(dades[,k]), col=paleta, lty=2, cex=0.6)
    
    #amb variable en eix d'abcisses
    marg <-table(dades[,k])/n
    print(append("Categories=",levels(dades[,k])))
    plot(marg,type="l",ylim=c(0,1),main=paste("Prop. of pos & neg by",names(dades)[k]))
    paleta<-rainbow(length(levels(as.factor(P))))
    for(c in 1:length(levels(as.factor(P)))){lines(rowperc[c,],col=paleta[c]) }
    
    #with legend
    plot(marg,type="l",ylim=c(0,1),main=paste("Prop. of pos & neg by",names(dades)[k]))
    for(c in 1:length(levels(as.factor(P)))){lines(rowperc[c,],col=paleta[c])}
    legend("topright", levels(as.factor(P)), col=paleta, lty=2, cex=0.6)
    
    #condicionades a columna 
    plot(marg,type="n",ylim=c(0,1),main=paste("Prop. of pos & neg by",names(dades)[k]))
    paleta<-rainbow(length(levels(as.factor(P))))
    for(c in 1:length(levels(as.factor(P)))){lines(colperc[c,],col=paleta[c]) }
    
    #with legend
    plot(marg,type="n",ylim=c(0,1),main=paste("Prop. of pos & neg by",names(dades)[k]))
    for(c in 1:length(levels(as.factor(P)))){lines(colperc[c,],col=paleta[c])}
    legend("topright", levels(as.factor(P)), col=paleta, lty=2, cex=0.6)
    
    table<-table(dades[,k],P)
    print("Cross Table:")
    print(table)
    print("Distribucions condicionades a columnes:")
    print(colperc)
    
    #diagrames de barres apilades                                         
    
    paleta<-rainbow(length(levels(dades[,k])))
    barplot(table(dades[,k], as.factor(P)), beside=FALSE,col=paleta )
    
    barplot(table(dades[,k], as.factor(P)), beside=FALSE,col=paleta )
    legend("topright",levels(as.factor(dades[,k])),pch=1,cex=0.5, col=paleta)
    
    #diagrames de barres adosades
    barplot(table(dades[,k], as.factor(P)), beside=TRUE,col=paleta )
    
    barplot(table(dades[,k], as.factor(P)), beside=TRUE,col=paleta)
    legend("topright",levels(as.factor(dades[,k])),pch=1,cex=0.5, col=paleta)
    
    print("Test Chi quadrat: ")
    print(chisq.test(dades[,k], as.factor(P)))
    
    print("valorsTest:")
    print( ValorTestXquali(P,dades[,k]))
  }
}#endfor




for (c in 1:length(levels(as.factor(P)))) { 
    if(!is.na(levels(as.factor(P))[c])){print(paste("P.values per 
        class:",levels(as.factor(P))[c])); print(sort(pvalk[c,]), digits=3) }}



\end{verbatim}

\end{document}